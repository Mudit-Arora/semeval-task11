# -*- coding: utf-8 -*-
"""SemEval_Task_11_Multilingual_Emotion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sGt-gn8_AT_pH_GLAn5cpQeR8nEI_L6v
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
import gensim.downloader as api
from typing import Optional, Dict, List
import matplotlib.pyplot as plt
from pathlib import Path
from tqdm import tqdm

def preprocess_data(source_df, target_df, emotion_cols):

    """
    We are starting off by pre-processing the  data. The main thing to keep in mind here is that we are ensuring that both
     datasets are being  pre-processed separately since the max length of each text is different.

     The actual pre-processing steps are fairly straigthtforward, we are normalizing the text length  and then cleaning the text
     by removing the whitespace.

     In future approaches this process will be built upon as it is fairly basic currently.
    """

    source_df = source_df[source_df[emotion_cols].sum(axis=1) > 0].reset_index(drop=True)

    source_df['text'] = source_df['text'].str[:140]
    target_df['text'] = target_df['text'].str[:80]

    source_df['text'] = source_df['text'].str.replace(r'\s+', ' ').str.strip()
    target_df['text'] = target_df['text'].str.replace(r'\s+', ' ').str.strip()

    return source_df, target_df

def calculate_class_weights(df, emotion_cols):

    """
    This function is simply here to calculate the class weights for the  dataset. We already know that the dataset is imbalanced class wise
    and so this is being used to find those weights and then they are being applied later in our  loss function.
    """

    total_samples = len(df)
    pos_samples = df[emotion_cols].sum().values
    neg_samples = total_samples - pos_samples
    weights = np.clip(neg_samples / pos_samples, a_min=1, a_max=10)

    return torch.FloatTensor(weights)

def load_multilingual_embeddings(vocab, source_lang='deu', target_lang='rus'):

    """
    Here is where we are loading in our embeddings. I have realized now that this is not properly implemented and for future
    work this will need to be fixed. The process of fixing is a lot more complex than I had initially expected, so I did not
    have time to fix it before the deadlines of the project.

    The actual process is fairly simple, we are loading in the embeddings and then creating an embedding matrix based on the
    vocabulary. It is a similar process to what we have done in previous assignments for the class.
    """

    ft_model = api.load('fasttext-wiki-news-subwords-300')
    embedding_dim = 300
    embedding_matrix = np.zeros((len(vocab), embedding_dim))

    embedding_matrix[0] = np.zeros(embedding_dim)
    embedding_matrix[1] = np.mean([ft_model.get_vector(w) for w in list(ft_model.index_to_key)[:1000]], axis=0)

    for word, idx in vocab.items():
        if word in ['<PAD>', '<UNK>']:
            continue
        try:
            embedding_matrix[idx] = ft_model.get_vector(word)
        except KeyError:
            embedding_matrix[idx] = embedding_matrix[1]

    return embedding_matrix



def build_multilingual_vocab(texts_by_lang: Dict[str, List[str]], min_freq: int = 3):

    """
    There is not too much to say about this. I have built similar functions for the last two assignments.
    This is not that much different from the vocabulary function I built for the last assignment. The only different is that it
    should work better for multilingual assignments.
    """

    vocab = {'<PAD>': 0, '<UNK>': 1}
    word_freq = {}
    idx = 2

    for texts in texts_by_lang.values():
        for text in texts:
            words = str(text).lower().split()
            for word in words:
                word_freq[word] = word_freq.get(word, 0) + 1

    for word, freq in word_freq.items():
        if freq >= min_freq:
            vocab[word] = idx
            idx += 1

    return vocab

def collate_fn(batch):

    """
    A fairly straightforward collate function for batch progressing. This is something I generally just include now in every
    assignment related to deep  learning since it tends to be a useful addition. I don't really think there is anything in particular
    I am doing in this that is particularly special. The main difference is that it is built for multi-class classification rather than
    tagging or language modeling.
    """

    batch = sorted(batch, key=lambda x: x['length'], reverse=True)
    max_len = max(x['length'] for x in batch)

    texts = torch.stack([x['text'][:max_len] for x in batch])
    lengths = torch.tensor([x['length'] for x in batch])
    lang_codes = [x['lang_code'] for x in batch]

    output = {
        'text': texts,
        'length': lengths,
        'lang_code': lang_codes
    }

    if 'labels' in batch[0]:
        labels = torch.stack([x['labels'] for x in batch])
        output['labels'] = labels

    return output

class MultilingualEmotionDataset(Dataset):

    """
    This is a fairly straightforward dataset class. The main differences between this and previous datasets I have built is that it needs
    to be aware of the specific language being utilized. Outside of that it is following the same structure as the dataset I have made
    for assignment 3.
    """

    def __init__(self,
                 texts: List[str],
                 labels: Optional[np.ndarray] = None,
                 vocab: Dict = None,
                 lang_code: str = 'deu',
                 max_len: int = 140):
        self.texts = texts
        self.labels = labels
        self.vocab = vocab
        self.lang_code = lang_code
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        words = str(text).lower().split()

        indices = []
        for word in words[:self.max_len]:
            indices.append(self.vocab.get(word, self.vocab['<UNK>']))

        if len(indices) < self.max_len:
            indices.extend([self.vocab['<PAD>']] * (self.max_len - len(indices)))
        else:
            indices = indices[:self.max_len]

        output = {
            'text': torch.LongTensor(indices),
            'length': min(len(words), self.max_len),
            'lang_code': self.lang_code
        }

        if self.labels is not None:
            output['labels'] = torch.FloatTensor(self.labels[idx])

        return output


class FocalLoss(nn.Module):

    """
    Realistically this class probably is not  actually that necessary. It is not hugely different from the BCE loss with Logists function
    already built into torch. I am just keeping it included since it isn't causing any significant harm either.
    """

    def __init__(self, gamma=2, alpha=None):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha

    def forward(self, inputs, targets):
        bce_loss = F.binary_cross_entropy_with_logits(
            inputs,
            targets.float(),
            pos_weight=self.alpha,
            reduction='none'
        )

        probs = torch.sigmoid(inputs)
        pt = torch.where(targets == 1, probs, 1 - probs)
        focal_loss = ((1 - pt) ** self.gamma * bce_loss)

        return focal_loss.mean()

class LanguageAwareAttention(nn.Module):

    """
    This class is entirely just for ensuring the attention mechanism is working properly. It  could have been included in
    the base architecture instead, but I also did not want to make the base architecture too complex. And also seperating it makes it
     more useful for debugging and making changes specifically to attention when necessary.
     """

    def __init__(self, hidden_dim):
        super().__init__()
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, lstm_output, mask):
        attention_weights = self.attention(lstm_output)
        attention_weights = attention_weights.squeeze(-1)
        attention_weights = attention_weights.masked_fill(~mask, float('-inf'))
        attention_weights = F.softmax(attention_weights, dim=1)
        attended = torch.bmm(attention_weights.unsqueeze(1), lstm_output)
        return attended.squeeze(1)

class MultilingualBiLSTM(nn.Module):

    """
    This is the main model class. It is a variable layer BiLSTM model with attention mechanism included, and the option to use
    pretrained embeddings if applicable. Otherwise it relies on basic embeddings built using torch's nn.Embedding class.

    The model should in theory be able to handle multiple languages, and has done so for the different tests I have run with varying results.
    There is definitely a lot that could be improved with this, but for the final SemEval task I will be utilizing transformers instead.
    However I may still try and build up on this BiLSTM model as I do believe that it can produce better results with some additional work.
    """

    def __init__(self,
                 vocab_size: int,
                 embedding_dim: int,
                 hidden_dim: int,
                 num_emotions: int,
                 num_layers: int = 2,
                 dropout: float = 0.5,
                 pretrained_embeddings: Optional[np.ndarray] = None):
        super().__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        if pretrained_embeddings is not None:
            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))

        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if num_layers > 1 else 0
        )

        self.attention = LanguageAwareAttention(hidden_dim)

        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, num_emotions)
        )

        for name, param in self.named_parameters():
            if 'weight' in name and 'embedding' not in name:
                nn.init.xavier_uniform_(param)

    def forward(self, text, lengths):
        embedded = self.embedding(text)

        mask = torch.arange(text.size(1), device=text.device)[None, :] < lengths[:, None]
        mask = mask.to(text.device)

        packed = pack_padded_sequence(
            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False
        )

        lstm_out, _ = self.lstm(packed)
        unpacked, _ = pad_packed_sequence(lstm_out, batch_first=True)

        attended = self.attention(unpacked, mask)
        output = self.classifier(attended)
        return output

def train_cross_lingual(model, train_loader, val_loaders, criterion, optimizer, scheduler,
                        device, num_epochs, model_dir):

    """

    This is the main training function for the model. It is fairly straightforward and I would not say there is anything specific to this
    that I feel is worth addressing. Note that there is a history dictionary that is being used to store the training and validation history
    so that it could be graphed. I have removed the function that utilized this since it was not necessary for the final code submission but
    kept the dictionary in case it ends up needing to be re-implemented, as I have an additional copy of the code with the graphing
    function included.
    """

    model = model.to(device)
    best_val_f1 = {lang: 0 for lang in val_loaders.keys()}
    best_model_path = None
    history = {'train_loss': [], 'train_f1': [], 'val_loss': {}, 'val_f1': {}}

    for lang in val_loaders.keys():
        history['val_loss'][lang] = []
        history['val_f1'][lang] = []

    for epoch in range(num_epochs):

        model.train()
        total_train_loss = 0
        all_train_preds = []
        all_train_labels = []

        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):
            texts = batch['text'].to(device)
            lengths = batch['length'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()
            outputs = model(texts, lengths)
            loss = criterion(outputs, labels)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            total_train_loss += loss.item()

            preds = (torch.sigmoid(outputs) > 0.3).float()
            all_train_preds.append(preds.cpu().numpy())
            all_train_labels.append(labels.cpu().numpy())

        train_preds = np.vstack(all_train_preds)
        train_labels = np.vstack(all_train_labels)
        train_f1 = f1_score(train_labels, train_preds, average='micro')
        avg_train_loss = total_train_loss / len(train_loader)

        history['train_loss'].append(avg_train_loss)
        history['train_f1'].append(train_f1)

        model.eval()
        val_metrics = {}

        with torch.no_grad():
            for lang, val_loader in val_loaders.items():
                all_val_preds = []
                all_val_labels = []
                total_val_loss = 0

                for batch in val_loader:
                    texts = batch['text'].to(device)
                    lengths = batch['length'].to(device)
                    labels = batch['labels'].to(device)

                    outputs = model(texts, lengths)
                    loss = criterion(outputs, labels)
                    total_val_loss += loss.item()

                    preds = (torch.sigmoid(outputs) > 0.3).float()
                    all_val_preds.append(preds.cpu().numpy())
                    all_val_labels.append(labels.cpu().numpy())

                val_preds = np.vstack(all_val_preds)
                val_labels = np.vstack(all_val_labels)
                val_f1 = f1_score(val_labels, val_preds, average='micro')

                val_metrics[lang] = {
                    'loss': total_val_loss / len(val_loader),
                    'f1': val_f1
                }

                history['val_loss'][lang].append(val_metrics[lang]['loss'])
                history['val_f1'][lang].append(val_f1)

                if val_f1 > best_val_f1[lang]:
                    best_val_f1[lang] = val_f1
                    best_model_path = model_dir / f'model_{lang}_epoch_{epoch + 1}.pt'

                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'train_f1': train_f1,
                        'val_f1': val_f1,
                    }, best_model_path)

        print(f'\nEpoch {epoch + 1}/{num_epochs}:')
        print(f'Train Loss: {avg_train_loss:.4f}, Train F1: {train_f1:.4f}')
        for lang, metrics in val_metrics.items():
            print(f'{lang} - Val Loss: {metrics["loss"]:.4f}, Val F1: {metrics["f1"]:.4f}')

        avg_val_loss = np.mean([m['loss'] for m in val_metrics.values()])
        scheduler.step(avg_val_loss)

    return model, best_model_path, history


"""
Two very basic functions for running the final predictions and generating the submission.csv file.
"""

def predict(model, test_loader, device):
    model.eval()
    all_preds = []

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Generating predictions"):
            texts = batch['text'].to(device)
            lengths = batch['length'].to(device)

            outputs = model(texts, lengths)
            preds = (torch.sigmoid(outputs) > 0.3).float()
            all_preds.append(preds.cpu().numpy())

    return np.vstack(all_preds)


def create_submission(predictions, test_df, output_file, emotion_cols):
    predictions = predictions.astype(int)
    submission_df = pd.DataFrame(
        predictions,
        columns=emotion_cols,
        index=test_df.index
    )
    submission_df.insert(0, 'id', test_df['id'])
    submission_df.to_csv(output_file, index=False)
    return submission_df

def main():

    source_df = pd.read_csv('deu.csv')
    target_df = pd.read_csv('rus_c.csv')

    output_file = 'pred_rus_c.csv'
    model_dir = Path('models')
    source_lang = 'de'
    target_lang = 'ru'
    embedding_dim = 300
    hidden_dim = 512
    num_layers = 4
    dropout = 0.5
    batch_size = 32
    epochs = 10
    lr = 0.001

    model_dir.mkdir(exist_ok=True, parents=True)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    emotion_cols = ['Anger', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise']
    source_df, target_df = preprocess_data(source_df, target_df, emotion_cols)

    texts_by_lang = {
        source_lang: source_df['text'].tolist(),
        target_lang: target_df['text'].tolist()
    }
    vocab = build_multilingual_vocab(texts_by_lang, min_freq=3)

    pretrained_embeddings = load_multilingual_embeddings(vocab, source_lang, target_lang)

    train_texts, val_texts, train_labels, val_labels = train_test_split(
        source_df['text'].values,
        source_df[emotion_cols].values,
        test_size=0.15,
        random_state=42,
        stratify=source_df[emotion_cols].sum(axis=1)
    )

    train_dataset = MultilingualEmotionDataset(train_texts, train_labels, vocab, source_lang)
    val_dataset = MultilingualEmotionDataset(val_texts, val_labels, vocab, source_lang)
    test_dataset = MultilingualEmotionDataset(target_df['text'].values, None, vocab, target_lang)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

    class_weights = calculate_class_weights(source_df, emotion_cols)

    model = MultilingualBiLSTM(
        vocab_size=len(vocab),
        embedding_dim=embedding_dim,
        hidden_dim=hidden_dim,
        num_emotions=len(emotion_cols),
        num_layers=num_layers,
        dropout=dropout,
        pretrained_embeddings=pretrained_embeddings
    ).to(device)

    criterion = FocalLoss(gamma=2, alpha=class_weights.to(device))
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.1)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=2, verbose=True
    )

    val_loaders = {source_lang: val_loader}

    model, best_model_path, history = train_cross_lingual(
        model=model,
        train_loader=train_loader,
        val_loaders=val_loaders,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        num_epochs=epochs,
        model_dir=model_dir
    )

    if best_model_path and best_model_path.exists():
        checkpoint = torch.load(best_model_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])


    predictions = predict(model, test_loader, device)
    submission_df = pd.DataFrame(
        predictions.astype(int),
        columns=emotion_cols,
        index=target_df.index
    )
    submission_df.insert(0, 'id', target_df['id'])
    submission_df.to_csv(output_file, index=False)

if __name__ == '__main__':
    main()
