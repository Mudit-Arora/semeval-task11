# -*- coding: utf-8 -*-
"""SemEval C.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lp2J-wDta3iczEpjBs4_H_csDBe1M6hd
"""

def main():

    import pandas as pd
    import numpy as np
    import torch
    from torch import nn
    from torch.utils.data import Dataset, DataLoader
    from sklearn.metrics import classification_report, multilabel_confusion_matrix
    from collections import Counter
    import torch.nn.functional as F

    train_file = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Sem Eval/public_data/train/track_a/rus.csv")
    dev_file = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Sem Eval/public_data/dev/track_c/rus_c.csv")

    train_file

    dev_file

    # Convert labels to binary
    emotion_columns = ['Anger', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise']
    train_file[emotion_columns] = train_file[emotion_columns].astype(float)
    dev_file[emotion_columns] = dev_file[emotion_columns].fillna(0).astype(float)

    # Verify labels are binary
    print("Training labels range:",
          train_file[emotion_columns].values.min(),
          train_file[emotion_columns].values.max())
    print("Dev labels range:",
          dev_file[emotion_columns].values.min(),
          dev_file[emotion_columns].values.max())

    import pandas as pd
    from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
    import torch
    from tqdm import tqdm

    # Load the model and tokenizer
    model_name = "facebook/m2m100_418M"  # You can also use "facebook/m2m100_1.2B" for better but slower translation
    tokenizer = M2M100Tokenizer.from_pretrained(model_name)
    model = M2M100ForConditionalGeneration.from_pretrained(model_name)

    # Move model to GPU if available
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    def translate_text(text, model, tokenizer, device):
        # Set source language
        tokenizer.src_lang = "ru"

        # Tokenize and translate
        encoded = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
        encoded = {k: v.to(device) for k, v in encoded.items()}

        # Generate translation
        generated_tokens = model.generate(
            **encoded,
            forced_bos_token_id=tokenizer.get_lang_id("en"),
            max_length=512
        )

        # Decode the translation
        return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]

    # Load datasets
    train_file = pd.read_csv("rus.csv")
    dev_file = pd.read_csv("rus_c.csv")

    # Create copies of the dataframes with translated text
    train_translated = train_file.copy()
    dev_translated = dev_file.copy()

    # Translate training data
    print("Translating training data...")
    train_translations = []
    for text in tqdm(train_file['text']):
        translated = translate_text(text, model, tokenizer, device)
        train_translations.append(translated)
    train_translated['text'] = train_translations

    # Translate development data
    print("\nTranslating development data...")
    dev_translations = []
    for text in tqdm(dev_file['text']):
        translated = translate_text(text, model, tokenizer, device)
        dev_translations.append(translated)
    dev_translated['text'] = dev_translations

    # Save translated datasets
    train_translated.to_csv('rus_translated.csv', index=False)
    dev_translated.to_csv('rus_c_translated.csv', index=False)

    # Print some examples to verify translations
    print("\nSample translations from training data:")
    for i in range(min(5, len(train_file))):
        print(f"\nOriginal: {train_file['text'].iloc[i]}")
        print(f"Translated: {train_translations[i]}")

    print("\nSample translations from development data:")
    for i in range(min(5, len(dev_file))):
        print(f"\nOriginal: {dev_file['text'].iloc[i]}")
        print(f"Translated: {dev_translations[i]}")

    # Print translation statistics
    print("\nTranslation Statistics:")
    print(f"Training samples translated: {len(train_translations)}")
    print(f"Development samples translated: {len(dev_translations)}")

    train_file = pd.read_csv("rus_translated.csv")
    dev_file = pd.read_csv("rus_c_translated.csv")

    print(train_file)

    print(dev_file)

    class EmotionDataset(Dataset):
        def __init__(self, texts, labels=None, tokenizer=None, max_len=100):
            self.texts = texts
            self.labels = labels  # Labels can be None for prediction
            self.tokenizer = tokenizer
            self.max_len = max_len

        def __len__(self):
            return len(self.texts)

        def __getitem__(self, idx):
            text = str(self.texts[idx])
            words = text.lower().split()
            word_ids = []
            for word in words[:self.max_len]:
                if word in self.tokenizer:
                    word_ids.append(self.tokenizer[word])
                else:
                    word_ids.append(self.tokenizer['<UNK>'])

            padding_length = self.max_len - len(word_ids)
            if padding_length > 0:
                word_ids = word_ids + [self.tokenizer['<PAD>']] * padding_length
            else:
                word_ids = word_ids[:self.max_len]

            item = {'input_ids': torch.tensor(word_ids, dtype=torch.long)}
            if self.labels is not None:
                item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)
            return item

    class EmotionClassifier(nn.Module):
        def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, dropout=0.5):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
            self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2,
                               batch_first=True, bidirectional=True, dropout=dropout)
            self.fc = nn.Linear(hidden_dim * 2, num_classes)
            self.dropout = nn.Dropout(dropout)

        def forward(self, x):
            embedded = self.dropout(self.embedding(x))
            output, (hidden, cell) = self.lstm(embedded)
            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)
            return torch.sigmoid(self.fc(hidden))

    def create_vocabulary(df, min_freq=2):
        word_counts = Counter()
        for text in df['text']:
            word_counts.update(str(text).lower().split())

        vocabulary = {'<PAD>': 0, '<UNK>': 1}
        for word, count in word_counts.items():
            if count >= min_freq:
                vocabulary[word] = len(vocabulary)
        return vocabulary

    def predict_emotions(model, data_loader, device):
        model.eval()
        all_predictions = []

        with torch.no_grad():
            for batch in data_loader:
                input_ids = batch['input_ids'].to(device)
                outputs = model(input_ids)
                predictions = (outputs > 0.5).float()
                all_predictions.extend(predictions.cpu().numpy())

        return np.array(all_predictions)

    # Training setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    BATCH_SIZE = 32
    EMBEDDING_DIM = 128
    HIDDEN_DIM = 256
    LEARNING_RATE = 0.001

    # Create vocabulary from training data
    vocab = create_vocabulary(train_file)
    vocab_size = len(vocab)
    print(f"Vocabulary size: {vocab_size}")

    # Prepare datasets
    train_dataset = EmotionDataset(
        train_file['text'].values,
        train_file[['Anger', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise']].values,
        vocab
    )
    dev_dataset = EmotionDataset(
        dev_file['text'].values,
        tokenizer=vocab
    )

    # Create dataloaders
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE)

    # Train model
    model = EmotionClassifier(
        vocab_size=vocab_size,
        embedding_dim=EMBEDDING_DIM,
        hidden_dim=HIDDEN_DIM,
        num_classes=6
    ).to(device)

    criterion = nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # Training loop
    num_epochs = 5
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0

        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids)
            loss = criterion(outputs, labels)

            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}")

    # Predict emotions for development set
    predictions = predict_emotions(model, dev_loader, device)

    # Add predictions to development file
    emotion_columns = ['Anger', 'Disgust','Fear', 'Joy', 'Sadness', 'Surprise']
    for i, col in enumerate(emotion_columns):
        dev_file[col] = predictions[:, i]

    # Save the predictions
    output_path = "pred_rus_c.csv"
    dev_file.to_csv(output_path, index=False)
    print(f"\nPredictions saved to: {output_path}")

    # Print sample predictions
    print("\nSample predictions:")
    print(dev_file[['text'] + emotion_columns].head())

    # Print prediction statistics
    print("\nPrediction statistics:")
    for col in emotion_columns:
        print(f"\n{col} distribution:")
        print(dev_file[col].value_counts())

    def calculate_metrics(model, data_loader, device):
        model.eval()
        all_predictions = []
        all_labels = []

        with torch.no_grad():
            for batch in data_loader:
                input_ids = batch['input_ids'].to(device)
                labels = batch['labels'].to(device)

                outputs = model(input_ids)
                predictions = (outputs > 0.5).float()

                all_predictions.extend(predictions.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

        all_predictions = np.array(all_predictions)
        all_labels = np.array(all_labels)

        # Print classification report for each emotion
        emotion_columns = ['Anger', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise']
        print("\nClassification Report:")
        print(classification_report(all_labels, all_predictions, target_names=emotion_columns))

        # Calculate and print F1 scores for each emotion
        print("\nF1 Scores per Emotion:")
        for i, emotion in enumerate(emotion_columns):
            f1 = f1_score(all_labels[:, i], all_predictions[:, i], zero_division=0)
            print(f"{emotion}: {f1:.4f}")

        # Calculate macro and weighted average F1
        macro_f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)
        weighted_f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)

        print(f"\nMacro-average F1: {macro_f1:.4f}")
        print(f"Weighted-average F1: {weighted_f1:.4f}")

        return macro_f1

    # Add these imports at the top
    from sklearn.metrics import f1_score, classification_report

    # After training, evaluate on training set
    print("\nTraining Set Metrics:")
    train_metrics = calculate_metrics(model, train_loader, device)

if __name__ == '__main__':
    main()